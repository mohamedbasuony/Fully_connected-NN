{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72364cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from os import listdir\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6b7afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in directory:roses\n",
      "in directory:sunflowers\n",
      "in directory:daisy\n",
      "in directory:dandelion\n",
      "in directory:tulips\n"
     ]
    }
   ],
   "source": [
    "def load_directory(f_name):\n",
    "    #print (\"in file \" + f_name)\n",
    "    loaded_images = {}\n",
    "    labels_list = []\n",
    "    loaded_images_list = []\n",
    "    dir = os.listdir('/Users/mohamedbasuony/desktop/flower_photos')\n",
    "    if '.DS_Store' in dir:\n",
    "        dir.remove('.DS_Store')\n",
    "    for filename in listdir('/Users/mohamedbasuony/desktop/flower_photos/' + f_name):\n",
    "        # load image\n",
    "        img_data = plt.imread('/Users/mohamedbasuony/desktop/flower_photos/' + f_name + '/' + filename)\n",
    "        new_img = cv2.resize(img_data,\n",
    "                             dsize=(32, 32),  # 32 x 32 images\n",
    "                             interpolation=cv2.INTER_CUBIC)\n",
    "        loaded_images[filename] = new_img\n",
    "        sorted_loaded_images = dict(\n",
    "            sorted(loaded_images.items(), key=lambda x: x[0].lower()))\n",
    "    for key in (sorted_loaded_images):\n",
    "        labels_list.append(f_name)\n",
    "        loaded_images_list.append(loaded_images[key])\n",
    "    ret = np.array(loaded_images_list)\n",
    "    labels = np.array(labels_list)\n",
    "    testing_batch = ret[-100:]\n",
    "    testing_batch_labels = labels[-100:]\n",
    "    validation_batch = ret[-200:-100]\n",
    "    validation_batch_labels = labels[-200:-100]\n",
    "    training_batch = ret[:-200].copy()\n",
    "    training_batch_labels = labels[:-200].copy()\n",
    "    return testing_batch, training_batch, testing_batch_labels, training_batch_labels, validation_batch,validation_batch_labels\n",
    "\n",
    "training_data = []\n",
    "testing_data = []\n",
    "training_labels = []\n",
    "testing_labels = []\n",
    "validation_data = []\n",
    "validation_labels=[]\n",
    "\n",
    "for directoryname in listdir('/Users/mohamedbasuony/desktop/flower_photos/'):\n",
    "    print('in directory:' + directoryname)\n",
    "    testing_batch, training_batch, testing_batch_labels, training_batch_labels,validation_batch,validation_batch_labels = load_directory(\n",
    "        directoryname)\n",
    "    testing_data.extend(testing_batch)\n",
    "    testing_labels.extend(testing_batch_labels)\n",
    "    training_data.extend(training_batch)\n",
    "    training_labels.extend(training_batch_labels)\n",
    "    \n",
    "    validation_data.extend(validation_batch)\n",
    "    validation_labels.extend(validation_batch_labels)\n",
    "    \n",
    "testing_data = np.array(testing_data)\n",
    "testing_data = np.reshape(testing_data , (testing_data.shape[0], -1))\n",
    "training_data = np.array(training_data)\n",
    "training_data = np.reshape(training_data , (training_data.shape[0], -1))\n",
    "training_labels = np.array(training_labels)\n",
    "testing_labels = np.array(testing_labels)\n",
    "\n",
    "validation_labels = np.array(validation_labels)\n",
    "validation_data = np.array (validation_data)\n",
    "validation_data = np.reshape(validation_data , (validation_data.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d0609f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4]\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "def mix_data(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "    return a , b\n",
    "\n",
    "np.random.seed(17) \n",
    "daisy,dandelion,roses,sunflowers,tulips = np.split(testing_data,5)\n",
    "training_data,training_labels=mix_data(training_data,training_labels)\n",
    "validation_data,validation_labels=mix_data(validation_data,validation_labels)\n",
    "\n",
    "training_labels[training_labels == 'daisy'] = 0\n",
    "training_labels[training_labels == 'dandelion'] = 1\n",
    "training_labels[training_labels == 'roses'] = 2\n",
    "training_labels[training_labels == 'sunflowers'] = 3\n",
    "training_labels[training_labels == 'tulips'] = 4\n",
    "\n",
    "testing_labels[testing_labels == 'daisy'] = 0\n",
    "testing_labels[testing_labels == 'dandelion'] = 1\n",
    "testing_labels[testing_labels == 'roses'] = 2\n",
    "testing_labels[testing_labels == 'sunflowers'] = 3\n",
    "testing_labels[testing_labels == 'tulips'] = 4\n",
    "\n",
    "validation_labels[validation_labels == 'daisy'] = 0\n",
    "validation_labels[validation_labels == 'dandelion'] = 1\n",
    "validation_labels[validation_labels == 'roses'] = 2\n",
    "validation_labels[validation_labels == 'sunflowers'] = 3\n",
    "validation_labels[validation_labels == 'tulips'] = 4\n",
    "\n",
    "training_labels = training_labels.astype('int32')\n",
    "testing_labels = testing_labels.astype('int32')\n",
    "validation_labels = validation_labels.astype('int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9afde849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n",
      "[[-4.40296930e-03 -4.41027494e-03 -3.47690537e-03 ... -3.18035633e-03\n",
      "  -3.10636137e-03 -2.96712823e-03]\n",
      " [ 3.19177733e-03 -1.54871235e-03 -2.91801609e-03 ...  2.20318566e-03\n",
      "  -2.30760655e-03 -2.79722290e-03]\n",
      " [ 4.72058996e-03  2.09327638e-03  9.01060705e-04 ... -2.64733237e-03\n",
      "  -2.82109179e-03 -1.55125049e-03]\n",
      " ...\n",
      " [-4.99476774e-03 -5.29475791e-03 -4.03579466e-03 ... -4.45961383e-03\n",
      "  -4.81797886e-03 -3.36357399e-03]\n",
      " [-3.17005589e-03 -2.17305328e-03 -2.26597858e-03 ... -2.59402997e-03\n",
      "  -1.68001347e-03 -2.00433137e-03]\n",
      " [ 3.55189887e-05  1.00067976e-03  2.02449097e-04 ... -2.43412278e-03\n",
      "  -2.19349871e-03 -1.94769626e-03]]\n",
      "iteration 0: loss 1.609268\n",
      "iteration 1000: loss 1.447209\n",
      "iteration 2000: loss 1.375594\n",
      "iteration 3000: loss 1.330926\n",
      "iteration 4000: loss 1.299614\n",
      "iteration 5000: loss 1.275928\n",
      "iteration 6000: loss 1.257045\n",
      "iteration 7000: loss 1.241413\n",
      "iteration 8000: loss 1.228103\n",
      "iteration 9000: loss 1.216522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w = 32 \n",
    "K = 5 \n",
    "D = w * w * 3 \n",
    "print (D)\n",
    "W =  np.random.randn(D,K)/ np.sqrt(D) \n",
    "b = np.zeros((1,K))\n",
    "step_size = 1e-0\n",
    "reg = 1e-6 \n",
    "X = training_data.astype('float64')\n",
    "y = training_labels\n",
    "X -= np.mean (X , axis = 0 )\n",
    "X /= np.std (X , axis = 0 )\n",
    "X/= 255.0\n",
    "print (X)\n",
    "num_examples = training_data.shape[0]\n",
    "all_loss=[]\n",
    "for i in range(10000):\n",
    "    scores = np.dot(X, W) + b\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) \n",
    "    correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(correct_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "    loss = data_loss + reg_loss\n",
    "    all_loss.append(loss)\n",
    "    if i % 1000 == 0:\n",
    "        print (\"iteration %d: loss %f\" % (i, loss))\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "\n",
    "    dW = np.dot(X.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "    dW += reg*W \n",
    "\n",
    "\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "505dbb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:Tr_loss: 2.607151, val_loss: 2.612527 ,Tr_acc: 0.220599 , val_acc: 0.210000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 1:Tr_loss: 2.596053, val_loss: 2.605775 ,Tr_acc: 0.263670 , val_acc: 0.226000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 2:Tr_loss: 2.585872, val_loss: 2.599448 ,Tr_acc: 0.285019 , val_acc: 0.264000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 3:Tr_loss: 2.576406, val_loss: 2.593419 ,Tr_acc: 0.299251 , val_acc: 0.290000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 4:Tr_loss: 2.567511, val_loss: 2.587607 ,Tr_acc: 0.314232 , val_acc: 0.304000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 5:Tr_loss: 2.559090, val_loss: 2.581947 ,Tr_acc: 0.323970 , val_acc: 0.306000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 6:Tr_loss: 2.551043, val_loss: 2.576398 ,Tr_acc: 0.336704 , val_acc: 0.312000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 7:Tr_loss: 2.543315, val_loss: 2.570946 ,Tr_acc: 0.346442 , val_acc: 0.314000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 8:Tr_loss: 2.535852, val_loss: 2.565561 ,Tr_acc: 0.356929 , val_acc: 0.316000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 9:Tr_loss: 2.528603, val_loss: 2.560228 ,Tr_acc: 0.361049 , val_acc: 0.328000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 10:Tr_loss: 2.521527, val_loss: 2.554927 ,Tr_acc: 0.368539 , val_acc: 0.334000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 11:Tr_loss: 2.514600, val_loss: 2.549655 ,Tr_acc: 0.377903 , val_acc: 0.328000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 12:Tr_loss: 2.507816, val_loss: 2.544417 ,Tr_acc: 0.386142 , val_acc: 0.340000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 13:Tr_loss: 2.501156, val_loss: 2.539220 ,Tr_acc: 0.391011 , val_acc: 0.348000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 14:Tr_loss: 2.494611, val_loss: 2.534047 ,Tr_acc: 0.397378 , val_acc: 0.354000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 15:Tr_loss: 2.488162, val_loss: 2.528902 ,Tr_acc: 0.402996 , val_acc: 0.352000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 16:Tr_loss: 2.481809, val_loss: 2.523797 ,Tr_acc: 0.405993 , val_acc: 0.354000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 17:Tr_loss: 2.475539, val_loss: 2.518724 ,Tr_acc: 0.411236 , val_acc: 0.362000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 18:Tr_loss: 2.469352, val_loss: 2.513686 ,Tr_acc: 0.416479 , val_acc: 0.370000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 19:Tr_loss: 2.463235, val_loss: 2.508689 ,Tr_acc: 0.419101 , val_acc: 0.370000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 20:Tr_loss: 2.457188, val_loss: 2.503727 ,Tr_acc: 0.422472 , val_acc: 0.372000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 21:Tr_loss: 2.451212, val_loss: 2.498805 ,Tr_acc: 0.424345 , val_acc: 0.368000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 22:Tr_loss: 2.445299, val_loss: 2.493920 ,Tr_acc: 0.425094 , val_acc: 0.372000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 23:Tr_loss: 2.439445, val_loss: 2.489078 ,Tr_acc: 0.427715 , val_acc: 0.370000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 24:Tr_loss: 2.433642, val_loss: 2.484278 ,Tr_acc: 0.430712 , val_acc: 0.372000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 25:Tr_loss: 2.427890, val_loss: 2.479519 ,Tr_acc: 0.433333 , val_acc: 0.370000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 26:Tr_loss: 2.422189, val_loss: 2.474808 ,Tr_acc: 0.433708 , val_acc: 0.370000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 27:Tr_loss: 2.416546, val_loss: 2.470159 ,Tr_acc: 0.435581 , val_acc: 0.374000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 28:Tr_loss: 2.410954, val_loss: 2.465559 ,Tr_acc: 0.438202 , val_acc: 0.378000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 29:Tr_loss: 2.405410, val_loss: 2.461007 ,Tr_acc: 0.441573 , val_acc: 0.380000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 30:Tr_loss: 2.399910, val_loss: 2.456504 ,Tr_acc: 0.444944 , val_acc: 0.382000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 31:Tr_loss: 2.394450, val_loss: 2.452051 ,Tr_acc: 0.447566 , val_acc: 0.384000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 32:Tr_loss: 2.389034, val_loss: 2.447649 ,Tr_acc: 0.452060 , val_acc: 0.384000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 33:Tr_loss: 2.383659, val_loss: 2.443302 ,Tr_acc: 0.457678 , val_acc: 0.382000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 34:Tr_loss: 2.378324, val_loss: 2.439011 ,Tr_acc: 0.461049 , val_acc: 0.386000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 35:Tr_loss: 2.373027, val_loss: 2.434766 ,Tr_acc: 0.462921 , val_acc: 0.384000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 36:Tr_loss: 2.367770, val_loss: 2.430567 ,Tr_acc: 0.467790 , val_acc: 0.386000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 37:Tr_loss: 2.362548, val_loss: 2.426424 ,Tr_acc: 0.470787 , val_acc: 0.382000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 38:Tr_loss: 2.357367, val_loss: 2.422334 ,Tr_acc: 0.473783 , val_acc: 0.384000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 39:Tr_loss: 2.352223, val_loss: 2.418299 ,Tr_acc: 0.476030 , val_acc: 0.386000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 40:Tr_loss: 2.347119, val_loss: 2.414317 ,Tr_acc: 0.477528 , val_acc: 0.390000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 41:Tr_loss: 2.342054, val_loss: 2.410389 ,Tr_acc: 0.480150 , val_acc: 0.390000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 42:Tr_loss: 2.337030, val_loss: 2.406519 ,Tr_acc: 0.480899 , val_acc: 0.396000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 43:Tr_loss: 2.332044, val_loss: 2.402707 ,Tr_acc: 0.484644 , val_acc: 0.394000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 44:Tr_loss: 2.327094, val_loss: 2.398952 ,Tr_acc: 0.487266 , val_acc: 0.396000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 45:Tr_loss: 2.322185, val_loss: 2.395254 ,Tr_acc: 0.489888 , val_acc: 0.398000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 46:Tr_loss: 2.317313, val_loss: 2.391611 ,Tr_acc: 0.491386 , val_acc: 0.398000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 47:Tr_loss: 2.312479, val_loss: 2.388028 ,Tr_acc: 0.493633 , val_acc: 0.402000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 48:Tr_loss: 2.307678, val_loss: 2.384494 ,Tr_acc: 0.497753 , val_acc: 0.404000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 49:Tr_loss: 2.302913, val_loss: 2.381009 ,Tr_acc: 0.499251 , val_acc: 0.406000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 50:Tr_loss: 2.298183, val_loss: 2.377569 ,Tr_acc: 0.500749 , val_acc: 0.406000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 51:Tr_loss: 2.293487, val_loss: 2.374181 ,Tr_acc: 0.503745 , val_acc: 0.410000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 52:Tr_loss: 2.288826, val_loss: 2.370842 ,Tr_acc: 0.505618 , val_acc: 0.414000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 53:Tr_loss: 2.284203, val_loss: 2.367554 ,Tr_acc: 0.507116 , val_acc: 0.420000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 54:Tr_loss: 2.279620, val_loss: 2.364320 ,Tr_acc: 0.509363 , val_acc: 0.422000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 55:Tr_loss: 2.275076, val_loss: 2.361136 ,Tr_acc: 0.508240 , val_acc: 0.422000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 56:Tr_loss: 2.270568, val_loss: 2.358003 ,Tr_acc: 0.508989 , val_acc: 0.428000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 57:Tr_loss: 2.266102, val_loss: 2.354926 ,Tr_acc: 0.511985 , val_acc: 0.432000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 58:Tr_loss: 2.261676, val_loss: 2.351901 ,Tr_acc: 0.516479 , val_acc: 0.430000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 59:Tr_loss: 2.257292, val_loss: 2.348929 ,Tr_acc: 0.519101 , val_acc: 0.432000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 60:Tr_loss: 2.252950, val_loss: 2.346013 ,Tr_acc: 0.519476 , val_acc: 0.438000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 61:Tr_loss: 2.248649, val_loss: 2.343146 ,Tr_acc: 0.521348 , val_acc: 0.442000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 62:Tr_loss: 2.244389, val_loss: 2.340329 ,Tr_acc: 0.523221 , val_acc: 0.442000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 63:Tr_loss: 2.240169, val_loss: 2.337563 ,Tr_acc: 0.523221 , val_acc: 0.444000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 64:Tr_loss: 2.235988, val_loss: 2.334852 ,Tr_acc: 0.524345 , val_acc: 0.450000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 65:Tr_loss: 2.231846, val_loss: 2.332194 ,Tr_acc: 0.525468 , val_acc: 0.448000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 66:Tr_loss: 2.227740, val_loss: 2.329586 ,Tr_acc: 0.528090 , val_acc: 0.446000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 67:Tr_loss: 2.223675, val_loss: 2.327023 ,Tr_acc: 0.529963 , val_acc: 0.446000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 68:Tr_loss: 2.219651, val_loss: 2.324516 ,Tr_acc: 0.532584 , val_acc: 0.448000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 69:Tr_loss: 2.215669, val_loss: 2.322052 ,Tr_acc: 0.532959 , val_acc: 0.448000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 70:Tr_loss: 2.211726, val_loss: 2.319636 ,Tr_acc: 0.534082 , val_acc: 0.448000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 71:Tr_loss: 2.207822, val_loss: 2.317274 ,Tr_acc: 0.535206 , val_acc: 0.448000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 72:Tr_loss: 2.203959, val_loss: 2.314962 ,Tr_acc: 0.535955 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 73:Tr_loss: 2.200137, val_loss: 2.312695 ,Tr_acc: 0.537453 , val_acc: 0.456000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 74:Tr_loss: 2.196356, val_loss: 2.310474 ,Tr_acc: 0.541199 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 75:Tr_loss: 2.192614, val_loss: 2.308300 ,Tr_acc: 0.541948 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 76:Tr_loss: 2.188912, val_loss: 2.306175 ,Tr_acc: 0.541948 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 77:Tr_loss: 2.185249, val_loss: 2.304100 ,Tr_acc: 0.543820 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 78:Tr_loss: 2.181625, val_loss: 2.302071 ,Tr_acc: 0.544944 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 79:Tr_loss: 2.178043, val_loss: 2.300086 ,Tr_acc: 0.545693 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 80:Tr_loss: 2.174498, val_loss: 2.298149 ,Tr_acc: 0.546816 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 81:Tr_loss: 2.170991, val_loss: 2.296249 ,Tr_acc: 0.549064 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 82:Tr_loss: 2.167519, val_loss: 2.294393 ,Tr_acc: 0.550187 , val_acc: 0.452000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 83:Tr_loss: 2.164084, val_loss: 2.292578 ,Tr_acc: 0.552060 , val_acc: 0.452000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 84:Tr_loss: 2.160684, val_loss: 2.290802 ,Tr_acc: 0.554307 , val_acc: 0.452000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 85:Tr_loss: 2.157320, val_loss: 2.289067 ,Tr_acc: 0.555056 , val_acc: 0.452000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 86:Tr_loss: 2.153989, val_loss: 2.287374 ,Tr_acc: 0.555805 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 87:Tr_loss: 2.150694, val_loss: 2.285723 ,Tr_acc: 0.556929 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 88:Tr_loss: 2.147432, val_loss: 2.284107 ,Tr_acc: 0.557678 , val_acc: 0.458000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 89:Tr_loss: 2.144203, val_loss: 2.282526 ,Tr_acc: 0.560674 , val_acc: 0.460000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 90:Tr_loss: 2.141006, val_loss: 2.280980 ,Tr_acc: 0.562547 , val_acc: 0.456000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 91:Tr_loss: 2.137841, val_loss: 2.279471 ,Tr_acc: 0.562172 , val_acc: 0.454000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 92:Tr_loss: 2.134706, val_loss: 2.278000 ,Tr_acc: 0.563670 , val_acc: 0.458000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 93:Tr_loss: 2.131601, val_loss: 2.276566 ,Tr_acc: 0.565543 , val_acc: 0.456000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 94:Tr_loss: 2.128527, val_loss: 2.275170 ,Tr_acc: 0.567416 , val_acc: 0.458000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 95:Tr_loss: 2.125482, val_loss: 2.273796 ,Tr_acc: 0.569663 , val_acc: 0.456000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 96:Tr_loss: 2.122466, val_loss: 2.272457 ,Tr_acc: 0.570037 , val_acc: 0.458000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 97:Tr_loss: 2.119476, val_loss: 2.271141 ,Tr_acc: 0.572659 , val_acc: 0.458000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 98:Tr_loss: 2.116514, val_loss: 2.269849 ,Tr_acc: 0.573783 , val_acc: 0.460000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 99:Tr_loss: 2.113579, val_loss: 2.268582 ,Tr_acc: 0.573783 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 100:Tr_loss: 2.110671, val_loss: 2.267345 ,Tr_acc: 0.575655 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 101:Tr_loss: 2.107788, val_loss: 2.266135 ,Tr_acc: 0.576404 , val_acc: 0.466000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 102:Tr_loss: 2.104933, val_loss: 2.264950 ,Tr_acc: 0.577903 , val_acc: 0.468000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 103:Tr_loss: 2.102099, val_loss: 2.263794 ,Tr_acc: 0.579775 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 104:Tr_loss: 2.099288, val_loss: 2.262668 ,Tr_acc: 0.581648 , val_acc: 0.466000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 105:Tr_loss: 2.096498, val_loss: 2.261561 ,Tr_acc: 0.583146 , val_acc: 0.468000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 106:Tr_loss: 2.093727, val_loss: 2.260477 ,Tr_acc: 0.583146 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 107:Tr_loss: 2.090976, val_loss: 2.259411 ,Tr_acc: 0.584644 , val_acc: 0.466000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 108:Tr_loss: 2.088244, val_loss: 2.258375 ,Tr_acc: 0.586891 , val_acc: 0.468000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 109:Tr_loss: 2.085531, val_loss: 2.257355 ,Tr_acc: 0.587640 , val_acc: 0.466000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 110:Tr_loss: 2.082837, val_loss: 2.256355 ,Tr_acc: 0.588390 , val_acc: 0.464000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 111:Tr_loss: 2.080160, val_loss: 2.255370 ,Tr_acc: 0.589139 , val_acc: 0.466000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 112:Tr_loss: 2.077500, val_loss: 2.254410 ,Tr_acc: 0.591386 , val_acc: 0.468000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 113:Tr_loss: 2.074855, val_loss: 2.253464 ,Tr_acc: 0.594007 , val_acc: 0.468000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 114:Tr_loss: 2.072228, val_loss: 2.252533 ,Tr_acc: 0.595880 , val_acc: 0.468000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 115:Tr_loss: 2.069616, val_loss: 2.251623 ,Tr_acc: 0.597378 , val_acc: 0.468000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 116:Tr_loss: 2.067018, val_loss: 2.250735 ,Tr_acc: 0.599625 , val_acc: 0.470000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 117:Tr_loss: 2.064436, val_loss: 2.249859 ,Tr_acc: 0.602622 , val_acc: 0.470000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 118:Tr_loss: 2.061869, val_loss: 2.249001 ,Tr_acc: 0.602996 , val_acc: 0.470000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 119:Tr_loss: 2.059315, val_loss: 2.248157 ,Tr_acc: 0.604494 , val_acc: 0.470000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 120:Tr_loss: 2.056774, val_loss: 2.247330 ,Tr_acc: 0.604869 , val_acc: 0.472000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 121:Tr_loss: 2.054246, val_loss: 2.246521 ,Tr_acc: 0.605618 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 122:Tr_loss: 2.051730, val_loss: 2.245722 ,Tr_acc: 0.605993 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 123:Tr_loss: 2.049228, val_loss: 2.244948 ,Tr_acc: 0.607116 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 124:Tr_loss: 2.046736, val_loss: 2.244185 ,Tr_acc: 0.608240 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 125:Tr_loss: 2.044256, val_loss: 2.243433 ,Tr_acc: 0.608614 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 126:Tr_loss: 2.041786, val_loss: 2.242696 ,Tr_acc: 0.609738 , val_acc: 0.480000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 127:Tr_loss: 2.039326, val_loss: 2.241968 ,Tr_acc: 0.611236 , val_acc: 0.480000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 128:Tr_loss: 2.036875, val_loss: 2.241259 ,Tr_acc: 0.613109 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 129:Tr_loss: 2.034433, val_loss: 2.240561 ,Tr_acc: 0.614232 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 130:Tr_loss: 2.032001, val_loss: 2.239872 ,Tr_acc: 0.616479 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 131:Tr_loss: 2.029578, val_loss: 2.239196 ,Tr_acc: 0.618352 , val_acc: 0.480000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 132:Tr_loss: 2.027162, val_loss: 2.238528 ,Tr_acc: 0.619850 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 133:Tr_loss: 2.024754, val_loss: 2.237872 ,Tr_acc: 0.622846 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 134:Tr_loss: 2.022354, val_loss: 2.237229 ,Tr_acc: 0.623221 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 135:Tr_loss: 2.019961, val_loss: 2.236596 ,Tr_acc: 0.623970 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 136:Tr_loss: 2.017574, val_loss: 2.235974 ,Tr_acc: 0.625843 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 137:Tr_loss: 2.015194, val_loss: 2.235358 ,Tr_acc: 0.627341 , val_acc: 0.472000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 138:Tr_loss: 2.012820, val_loss: 2.234754 ,Tr_acc: 0.628464 , val_acc: 0.472000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 139:Tr_loss: 2.010453, val_loss: 2.234158 ,Tr_acc: 0.630337 , val_acc: 0.472000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 140:Tr_loss: 2.008092, val_loss: 2.233573 ,Tr_acc: 0.630712 , val_acc: 0.472000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 141:Tr_loss: 2.005735, val_loss: 2.232994 ,Tr_acc: 0.631835 , val_acc: 0.472000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 142:Tr_loss: 2.003384, val_loss: 2.232431 ,Tr_acc: 0.632959 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 143:Tr_loss: 2.001036, val_loss: 2.231878 ,Tr_acc: 0.633708 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 144:Tr_loss: 1.998692, val_loss: 2.231333 ,Tr_acc: 0.634082 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 145:Tr_loss: 1.996352, val_loss: 2.230797 ,Tr_acc: 0.635955 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 146:Tr_loss: 1.994015, val_loss: 2.230261 ,Tr_acc: 0.637079 , val_acc: 0.480000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 147:Tr_loss: 1.991683, val_loss: 2.229741 ,Tr_acc: 0.638202 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 148:Tr_loss: 1.989354, val_loss: 2.229225 ,Tr_acc: 0.638577 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 149:Tr_loss: 1.987027, val_loss: 2.228719 ,Tr_acc: 0.639700 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 150:Tr_loss: 1.984702, val_loss: 2.228214 ,Tr_acc: 0.640824 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 151:Tr_loss: 1.982380, val_loss: 2.227718 ,Tr_acc: 0.642697 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 152:Tr_loss: 1.980060, val_loss: 2.227235 ,Tr_acc: 0.643071 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 153:Tr_loss: 1.977742, val_loss: 2.226753 ,Tr_acc: 0.643446 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 154:Tr_loss: 1.975426, val_loss: 2.226278 ,Tr_acc: 0.643820 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 155:Tr_loss: 1.973112, val_loss: 2.225806 ,Tr_acc: 0.644569 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 156:Tr_loss: 1.970799, val_loss: 2.225336 ,Tr_acc: 0.644944 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 157:Tr_loss: 1.968488, val_loss: 2.224879 ,Tr_acc: 0.646816 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 158:Tr_loss: 1.966179, val_loss: 2.224428 ,Tr_acc: 0.647940 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 159:Tr_loss: 1.963871, val_loss: 2.223983 ,Tr_acc: 0.649064 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 160:Tr_loss: 1.961566, val_loss: 2.223546 ,Tr_acc: 0.650562 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 161:Tr_loss: 1.959262, val_loss: 2.223114 ,Tr_acc: 0.649813 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 162:Tr_loss: 1.956959, val_loss: 2.222680 ,Tr_acc: 0.651311 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 163:Tr_loss: 1.954658, val_loss: 2.222248 ,Tr_acc: 0.652809 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 164:Tr_loss: 1.952356, val_loss: 2.221819 ,Tr_acc: 0.653933 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 165:Tr_loss: 1.950056, val_loss: 2.221401 ,Tr_acc: 0.653933 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 166:Tr_loss: 1.947754, val_loss: 2.220986 ,Tr_acc: 0.655056 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 167:Tr_loss: 1.945452, val_loss: 2.220573 ,Tr_acc: 0.656929 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 168:Tr_loss: 1.943150, val_loss: 2.220161 ,Tr_acc: 0.658801 , val_acc: 0.474000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 169:Tr_loss: 1.940849, val_loss: 2.219757 ,Tr_acc: 0.661049 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 170:Tr_loss: 1.938548, val_loss: 2.219358 ,Tr_acc: 0.661798 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 171:Tr_loss: 1.936247, val_loss: 2.218959 ,Tr_acc: 0.664419 , val_acc: 0.480000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 172:Tr_loss: 1.933946, val_loss: 2.218569 ,Tr_acc: 0.665169 , val_acc: 0.482000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 173:Tr_loss: 1.931644, val_loss: 2.218178 ,Tr_acc: 0.667041 , val_acc: 0.482000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 174:Tr_loss: 1.929341, val_loss: 2.217796 ,Tr_acc: 0.667416 , val_acc: 0.480000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 175:Tr_loss: 1.927038, val_loss: 2.217416 ,Tr_acc: 0.668165 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 176:Tr_loss: 1.924734, val_loss: 2.217037 ,Tr_acc: 0.669288 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 177:Tr_loss: 1.922428, val_loss: 2.216666 ,Tr_acc: 0.671161 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 178:Tr_loss: 1.920122, val_loss: 2.216294 ,Tr_acc: 0.673034 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 179:Tr_loss: 1.917815, val_loss: 2.215928 ,Tr_acc: 0.675655 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 180:Tr_loss: 1.915507, val_loss: 2.215560 ,Tr_acc: 0.676404 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 181:Tr_loss: 1.913198, val_loss: 2.215204 ,Tr_acc: 0.677903 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 182:Tr_loss: 1.910889, val_loss: 2.214837 ,Tr_acc: 0.679775 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 183:Tr_loss: 1.908578, val_loss: 2.214491 ,Tr_acc: 0.681648 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 184:Tr_loss: 1.906265, val_loss: 2.214125 ,Tr_acc: 0.682772 , val_acc: 0.480000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 185:Tr_loss: 1.903950, val_loss: 2.213784 ,Tr_acc: 0.683521 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 186:Tr_loss: 1.901634, val_loss: 2.213429 ,Tr_acc: 0.685768 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 187:Tr_loss: 1.899316, val_loss: 2.213086 ,Tr_acc: 0.686517 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 188:Tr_loss: 1.896995, val_loss: 2.212742 ,Tr_acc: 0.688015 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 189:Tr_loss: 1.894672, val_loss: 2.212409 ,Tr_acc: 0.689139 , val_acc: 0.478000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 190:Tr_loss: 1.892347, val_loss: 2.212078 ,Tr_acc: 0.691760 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 191:Tr_loss: 1.890020, val_loss: 2.211751 ,Tr_acc: 0.692135 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 192:Tr_loss: 1.887692, val_loss: 2.211430 ,Tr_acc: 0.693633 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 193:Tr_loss: 1.885362, val_loss: 2.211117 ,Tr_acc: 0.694382 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 194:Tr_loss: 1.883029, val_loss: 2.210807 ,Tr_acc: 0.696255 , val_acc: 0.476000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 195:Tr_loss: 1.880693, val_loss: 2.210499 ,Tr_acc: 0.698876 , val_acc: 0.480000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 196:Tr_loss: 1.878355, val_loss: 2.210203 ,Tr_acc: 0.699251 , val_acc: 0.482000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 197:Tr_loss: 1.876016, val_loss: 2.209911 ,Tr_acc: 0.700375 , val_acc: 0.484000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 198:Tr_loss: 1.873676, val_loss: 2.209618 ,Tr_acc: 0.701873 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 199:Tr_loss: 1.871334, val_loss: 2.209325 ,Tr_acc: 0.702996 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 200:Tr_loss: 1.868991, val_loss: 2.209027 ,Tr_acc: 0.704120 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 201:Tr_loss: 1.866646, val_loss: 2.208753 ,Tr_acc: 0.704120 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 202:Tr_loss: 1.864298, val_loss: 2.208469 ,Tr_acc: 0.705618 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 203:Tr_loss: 1.861946, val_loss: 2.208193 ,Tr_acc: 0.707865 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 204:Tr_loss: 1.859592, val_loss: 2.207913 ,Tr_acc: 0.708614 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 205:Tr_loss: 1.857235, val_loss: 2.207645 ,Tr_acc: 0.709738 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 206:Tr_loss: 1.854875, val_loss: 2.207376 ,Tr_acc: 0.710487 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 207:Tr_loss: 1.852514, val_loss: 2.207113 ,Tr_acc: 0.711236 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 208:Tr_loss: 1.850150, val_loss: 2.206856 ,Tr_acc: 0.712360 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 209:Tr_loss: 1.847784, val_loss: 2.206590 ,Tr_acc: 0.714232 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 210:Tr_loss: 1.845414, val_loss: 2.206329 ,Tr_acc: 0.715356 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 211:Tr_loss: 1.843041, val_loss: 2.206080 ,Tr_acc: 0.717228 , val_acc: 0.486000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 212:Tr_loss: 1.840667, val_loss: 2.205829 ,Tr_acc: 0.718727 , val_acc: 0.492000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 213:Tr_loss: 1.838289, val_loss: 2.205587 ,Tr_acc: 0.720599 , val_acc: 0.492000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 214:Tr_loss: 1.835907, val_loss: 2.205343 ,Tr_acc: 0.720974 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 215:Tr_loss: 1.833523, val_loss: 2.205101 ,Tr_acc: 0.722472 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 216:Tr_loss: 1.831134, val_loss: 2.204863 ,Tr_acc: 0.722846 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 217:Tr_loss: 1.828743, val_loss: 2.204632 ,Tr_acc: 0.725094 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 218:Tr_loss: 1.826351, val_loss: 2.204392 ,Tr_acc: 0.726592 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 219:Tr_loss: 1.823958, val_loss: 2.204164 ,Tr_acc: 0.727341 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 220:Tr_loss: 1.821564, val_loss: 2.203927 ,Tr_acc: 0.727715 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 221:Tr_loss: 1.819166, val_loss: 2.203707 ,Tr_acc: 0.729588 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 222:Tr_loss: 1.816766, val_loss: 2.203483 ,Tr_acc: 0.730337 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 223:Tr_loss: 1.814365, val_loss: 2.203261 ,Tr_acc: 0.731835 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 224:Tr_loss: 1.811961, val_loss: 2.203042 ,Tr_acc: 0.733333 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 225:Tr_loss: 1.809555, val_loss: 2.202822 ,Tr_acc: 0.734082 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 226:Tr_loss: 1.807146, val_loss: 2.202607 ,Tr_acc: 0.735206 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 227:Tr_loss: 1.804735, val_loss: 2.202407 ,Tr_acc: 0.735581 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 228:Tr_loss: 1.802323, val_loss: 2.202197 ,Tr_acc: 0.735955 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 229:Tr_loss: 1.799908, val_loss: 2.202000 ,Tr_acc: 0.737079 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 230:Tr_loss: 1.797492, val_loss: 2.201787 ,Tr_acc: 0.738202 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 231:Tr_loss: 1.795074, val_loss: 2.201601 ,Tr_acc: 0.738951 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 232:Tr_loss: 1.792655, val_loss: 2.201395 ,Tr_acc: 0.740824 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 233:Tr_loss: 1.790234, val_loss: 2.201215 ,Tr_acc: 0.743071 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 234:Tr_loss: 1.787812, val_loss: 2.201022 ,Tr_acc: 0.745693 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 235:Tr_loss: 1.785387, val_loss: 2.200852 ,Tr_acc: 0.747191 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 236:Tr_loss: 1.782961, val_loss: 2.200668 ,Tr_acc: 0.749064 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 237:Tr_loss: 1.780531, val_loss: 2.200491 ,Tr_acc: 0.750187 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 238:Tr_loss: 1.778100, val_loss: 2.200318 ,Tr_acc: 0.753184 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 239:Tr_loss: 1.775668, val_loss: 2.200136 ,Tr_acc: 0.754307 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 240:Tr_loss: 1.773233, val_loss: 2.199972 ,Tr_acc: 0.756929 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 241:Tr_loss: 1.770797, val_loss: 2.199808 ,Tr_acc: 0.758052 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 242:Tr_loss: 1.768358, val_loss: 2.199637 ,Tr_acc: 0.759925 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 243:Tr_loss: 1.765917, val_loss: 2.199484 ,Tr_acc: 0.760300 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 244:Tr_loss: 1.763474, val_loss: 2.199301 ,Tr_acc: 0.761798 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 245:Tr_loss: 1.761028, val_loss: 2.199156 ,Tr_acc: 0.762547 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 246:Tr_loss: 1.758579, val_loss: 2.198985 ,Tr_acc: 0.764045 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 247:Tr_loss: 1.756129, val_loss: 2.198844 ,Tr_acc: 0.765543 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 248:Tr_loss: 1.753676, val_loss: 2.198677 ,Tr_acc: 0.767790 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 249:Tr_loss: 1.751223, val_loss: 2.198550 ,Tr_acc: 0.769663 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 250:Tr_loss: 1.748768, val_loss: 2.198383 ,Tr_acc: 0.769663 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 251:Tr_loss: 1.746313, val_loss: 2.198265 ,Tr_acc: 0.771536 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 252:Tr_loss: 1.743857, val_loss: 2.198117 ,Tr_acc: 0.774157 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 253:Tr_loss: 1.741400, val_loss: 2.198003 ,Tr_acc: 0.776030 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 254:Tr_loss: 1.738942, val_loss: 2.197878 ,Tr_acc: 0.776779 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 255:Tr_loss: 1.736483, val_loss: 2.197756 ,Tr_acc: 0.778277 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 256:Tr_loss: 1.734023, val_loss: 2.197628 ,Tr_acc: 0.780524 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 257:Tr_loss: 1.731562, val_loss: 2.197520 ,Tr_acc: 0.780524 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 258:Tr_loss: 1.729099, val_loss: 2.197405 ,Tr_acc: 0.782022 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 259:Tr_loss: 1.726634, val_loss: 2.197289 ,Tr_acc: 0.783521 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 260:Tr_loss: 1.724170, val_loss: 2.197174 ,Tr_acc: 0.784644 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 261:Tr_loss: 1.721704, val_loss: 2.197074 ,Tr_acc: 0.788015 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 262:Tr_loss: 1.719236, val_loss: 2.196961 ,Tr_acc: 0.789139 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 263:Tr_loss: 1.716767, val_loss: 2.196859 ,Tr_acc: 0.790637 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 264:Tr_loss: 1.714298, val_loss: 2.196754 ,Tr_acc: 0.791386 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 265:Tr_loss: 1.711826, val_loss: 2.196652 ,Tr_acc: 0.792509 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 266:Tr_loss: 1.709352, val_loss: 2.196578 ,Tr_acc: 0.794007 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 267:Tr_loss: 1.706877, val_loss: 2.196478 ,Tr_acc: 0.794757 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 268:Tr_loss: 1.704400, val_loss: 2.196389 ,Tr_acc: 0.795506 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 269:Tr_loss: 1.701925, val_loss: 2.196299 ,Tr_acc: 0.796629 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 270:Tr_loss: 1.699449, val_loss: 2.196230 ,Tr_acc: 0.797378 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 271:Tr_loss: 1.696971, val_loss: 2.196146 ,Tr_acc: 0.798127 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 272:Tr_loss: 1.694492, val_loss: 2.196066 ,Tr_acc: 0.800375 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 273:Tr_loss: 1.692012, val_loss: 2.196003 ,Tr_acc: 0.802247 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 274:Tr_loss: 1.689531, val_loss: 2.195916 ,Tr_acc: 0.804869 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 275:Tr_loss: 1.687050, val_loss: 2.195864 ,Tr_acc: 0.805618 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 276:Tr_loss: 1.684568, val_loss: 2.195801 ,Tr_acc: 0.806742 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 277:Tr_loss: 1.682087, val_loss: 2.195724 ,Tr_acc: 0.808614 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 278:Tr_loss: 1.679604, val_loss: 2.195683 ,Tr_acc: 0.811610 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 279:Tr_loss: 1.677121, val_loss: 2.195609 ,Tr_acc: 0.811985 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 280:Tr_loss: 1.674638, val_loss: 2.195569 ,Tr_acc: 0.813483 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 281:Tr_loss: 1.672155, val_loss: 2.195494 ,Tr_acc: 0.814232 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 282:Tr_loss: 1.669673, val_loss: 2.195474 ,Tr_acc: 0.817228 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 283:Tr_loss: 1.667191, val_loss: 2.195402 ,Tr_acc: 0.817603 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 284:Tr_loss: 1.664708, val_loss: 2.195399 ,Tr_acc: 0.819476 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 285:Tr_loss: 1.662224, val_loss: 2.195316 ,Tr_acc: 0.819850 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 286:Tr_loss: 1.659742, val_loss: 2.195351 ,Tr_acc: 0.821723 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 287:Tr_loss: 1.657259, val_loss: 2.195270 ,Tr_acc: 0.823221 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 288:Tr_loss: 1.654779, val_loss: 2.195299 ,Tr_acc: 0.823970 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 289:Tr_loss: 1.652298, val_loss: 2.195238 ,Tr_acc: 0.825843 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 290:Tr_loss: 1.649816, val_loss: 2.195261 ,Tr_acc: 0.826592 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 291:Tr_loss: 1.647335, val_loss: 2.195238 ,Tr_acc: 0.828464 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 292:Tr_loss: 1.644854, val_loss: 2.195227 ,Tr_acc: 0.830337 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 293:Tr_loss: 1.642373, val_loss: 2.195228 ,Tr_acc: 0.832584 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 294:Tr_loss: 1.639891, val_loss: 2.195221 ,Tr_acc: 0.833333 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 295:Tr_loss: 1.637409, val_loss: 2.195212 ,Tr_acc: 0.835581 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 296:Tr_loss: 1.634929, val_loss: 2.195218 ,Tr_acc: 0.836330 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 297:Tr_loss: 1.632449, val_loss: 2.195214 ,Tr_acc: 0.836330 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 298:Tr_loss: 1.629971, val_loss: 2.195249 ,Tr_acc: 0.836704 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 299:Tr_loss: 1.627494, val_loss: 2.195244 ,Tr_acc: 0.838202 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 300:Tr_loss: 1.625018, val_loss: 2.195274 ,Tr_acc: 0.838951 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 301:Tr_loss: 1.622544, val_loss: 2.195314 ,Tr_acc: 0.840075 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 302:Tr_loss: 1.620071, val_loss: 2.195333 ,Tr_acc: 0.840824 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 303:Tr_loss: 1.617599, val_loss: 2.195377 ,Tr_acc: 0.841199 , val_acc: 0.514000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 304:Tr_loss: 1.615129, val_loss: 2.195403 ,Tr_acc: 0.841948 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 305:Tr_loss: 1.612660, val_loss: 2.195456 ,Tr_acc: 0.843446 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 306:Tr_loss: 1.610193, val_loss: 2.195486 ,Tr_acc: 0.844569 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 307:Tr_loss: 1.607729, val_loss: 2.195554 ,Tr_acc: 0.846442 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 308:Tr_loss: 1.605266, val_loss: 2.195586 ,Tr_acc: 0.846442 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 309:Tr_loss: 1.602804, val_loss: 2.195666 ,Tr_acc: 0.846442 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 310:Tr_loss: 1.600344, val_loss: 2.195710 ,Tr_acc: 0.847191 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 311:Tr_loss: 1.597885, val_loss: 2.195805 ,Tr_acc: 0.847940 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 312:Tr_loss: 1.595430, val_loss: 2.195840 ,Tr_acc: 0.849064 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 313:Tr_loss: 1.592976, val_loss: 2.195957 ,Tr_acc: 0.849813 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 314:Tr_loss: 1.590525, val_loss: 2.195979 ,Tr_acc: 0.851685 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 315:Tr_loss: 1.588075, val_loss: 2.196119 ,Tr_acc: 0.852809 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 316:Tr_loss: 1.585628, val_loss: 2.196153 ,Tr_acc: 0.854307 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 317:Tr_loss: 1.583183, val_loss: 2.196325 ,Tr_acc: 0.855805 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 318:Tr_loss: 1.580739, val_loss: 2.196343 ,Tr_acc: 0.858052 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 319:Tr_loss: 1.578296, val_loss: 2.196547 ,Tr_acc: 0.858801 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 320:Tr_loss: 1.575855, val_loss: 2.196586 ,Tr_acc: 0.859925 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 321:Tr_loss: 1.573416, val_loss: 2.196791 ,Tr_acc: 0.861049 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 322:Tr_loss: 1.570980, val_loss: 2.196825 ,Tr_acc: 0.862547 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 323:Tr_loss: 1.568547, val_loss: 2.197038 ,Tr_acc: 0.863670 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 324:Tr_loss: 1.566116, val_loss: 2.197081 ,Tr_acc: 0.864419 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 325:Tr_loss: 1.563687, val_loss: 2.197323 ,Tr_acc: 0.865543 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 326:Tr_loss: 1.561261, val_loss: 2.197384 ,Tr_acc: 0.867041 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 327:Tr_loss: 1.558838, val_loss: 2.197632 ,Tr_acc: 0.867790 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 328:Tr_loss: 1.556419, val_loss: 2.197688 ,Tr_acc: 0.868165 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 329:Tr_loss: 1.554002, val_loss: 2.197953 ,Tr_acc: 0.869288 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 330:Tr_loss: 1.551587, val_loss: 2.198038 ,Tr_acc: 0.870037 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 331:Tr_loss: 1.549175, val_loss: 2.198304 ,Tr_acc: 0.870787 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 332:Tr_loss: 1.546765, val_loss: 2.198365 ,Tr_acc: 0.871161 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 333:Tr_loss: 1.544359, val_loss: 2.198672 ,Tr_acc: 0.871910 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 334:Tr_loss: 1.541957, val_loss: 2.198726 ,Tr_acc: 0.872285 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 335:Tr_loss: 1.539558, val_loss: 2.199035 ,Tr_acc: 0.873783 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 336:Tr_loss: 1.537164, val_loss: 2.199073 ,Tr_acc: 0.874157 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 337:Tr_loss: 1.534772, val_loss: 2.199441 ,Tr_acc: 0.875281 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 338:Tr_loss: 1.532382, val_loss: 2.199456 ,Tr_acc: 0.877154 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 339:Tr_loss: 1.529996, val_loss: 2.199833 ,Tr_acc: 0.877903 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 340:Tr_loss: 1.527614, val_loss: 2.199854 ,Tr_acc: 0.878652 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 341:Tr_loss: 1.525237, val_loss: 2.200248 ,Tr_acc: 0.879401 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 342:Tr_loss: 1.522863, val_loss: 2.200240 ,Tr_acc: 0.881273 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 343:Tr_loss: 1.520493, val_loss: 2.200676 ,Tr_acc: 0.881648 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 344:Tr_loss: 1.518126, val_loss: 2.200644 ,Tr_acc: 0.883521 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 345:Tr_loss: 1.515764, val_loss: 2.201086 ,Tr_acc: 0.883895 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 346:Tr_loss: 1.513405, val_loss: 2.201068 ,Tr_acc: 0.885019 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 347:Tr_loss: 1.511051, val_loss: 2.201543 ,Tr_acc: 0.885768 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 348:Tr_loss: 1.508700, val_loss: 2.201496 ,Tr_acc: 0.886891 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 349:Tr_loss: 1.506355, val_loss: 2.201989 ,Tr_acc: 0.888390 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 350:Tr_loss: 1.504014, val_loss: 2.201981 ,Tr_acc: 0.890637 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 351:Tr_loss: 1.501677, val_loss: 2.202487 ,Tr_acc: 0.891011 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 352:Tr_loss: 1.499344, val_loss: 2.202455 ,Tr_acc: 0.891760 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 353:Tr_loss: 1.497017, val_loss: 2.203007 ,Tr_acc: 0.892884 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 354:Tr_loss: 1.494693, val_loss: 2.202973 ,Tr_acc: 0.893633 , val_acc: 0.512000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 355:Tr_loss: 1.492374, val_loss: 2.203546 ,Tr_acc: 0.895131 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 356:Tr_loss: 1.490057, val_loss: 2.203487 ,Tr_acc: 0.897378 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 357:Tr_loss: 1.487747, val_loss: 2.204100 ,Tr_acc: 0.897378 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 358:Tr_loss: 1.485441, val_loss: 2.204048 ,Tr_acc: 0.898876 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 359:Tr_loss: 1.483140, val_loss: 2.204683 ,Tr_acc: 0.900000 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 360:Tr_loss: 1.480842, val_loss: 2.204607 ,Tr_acc: 0.899625 , val_acc: 0.510000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 361:Tr_loss: 1.478551, val_loss: 2.205236 ,Tr_acc: 0.900749 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 362:Tr_loss: 1.476263, val_loss: 2.205174 ,Tr_acc: 0.900749 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 363:Tr_loss: 1.473982, val_loss: 2.205849 ,Tr_acc: 0.901873 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 364:Tr_loss: 1.471704, val_loss: 2.205758 ,Tr_acc: 0.901498 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 365:Tr_loss: 1.469433, val_loss: 2.206465 ,Tr_acc: 0.903745 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 366:Tr_loss: 1.467165, val_loss: 2.206356 ,Tr_acc: 0.903745 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 367:Tr_loss: 1.464905, val_loss: 2.207108 ,Tr_acc: 0.904869 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 368:Tr_loss: 1.462647, val_loss: 2.206994 ,Tr_acc: 0.905243 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 369:Tr_loss: 1.460395, val_loss: 2.207770 ,Tr_acc: 0.906742 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 370:Tr_loss: 1.458148, val_loss: 2.207667 ,Tr_acc: 0.906742 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 371:Tr_loss: 1.455908, val_loss: 2.208431 ,Tr_acc: 0.907865 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 372:Tr_loss: 1.453671, val_loss: 2.208328 ,Tr_acc: 0.907865 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 373:Tr_loss: 1.451440, val_loss: 2.209093 ,Tr_acc: 0.908240 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 374:Tr_loss: 1.449215, val_loss: 2.209034 ,Tr_acc: 0.908614 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 375:Tr_loss: 1.446997, val_loss: 2.209830 ,Tr_acc: 0.909363 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 376:Tr_loss: 1.444782, val_loss: 2.209731 ,Tr_acc: 0.910112 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 377:Tr_loss: 1.442575, val_loss: 2.210575 ,Tr_acc: 0.910861 , val_acc: 0.508000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 378:Tr_loss: 1.440371, val_loss: 2.210430 ,Tr_acc: 0.911985 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 379:Tr_loss: 1.438175, val_loss: 2.211324 ,Tr_acc: 0.913109 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 380:Tr_loss: 1.435983, val_loss: 2.211154 ,Tr_acc: 0.913109 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 381:Tr_loss: 1.433797, val_loss: 2.212071 ,Tr_acc: 0.914607 , val_acc: 0.506000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 382:Tr_loss: 1.431616, val_loss: 2.211887 ,Tr_acc: 0.913483 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 383:Tr_loss: 1.429442, val_loss: 2.212831 ,Tr_acc: 0.914607 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 384:Tr_loss: 1.427272, val_loss: 2.212657 ,Tr_acc: 0.913858 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 385:Tr_loss: 1.425109, val_loss: 2.213611 ,Tr_acc: 0.915730 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 386:Tr_loss: 1.422950, val_loss: 2.213436 ,Tr_acc: 0.916479 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 387:Tr_loss: 1.420799, val_loss: 2.214368 ,Tr_acc: 0.917603 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 388:Tr_loss: 1.418654, val_loss: 2.214219 ,Tr_acc: 0.917978 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 389:Tr_loss: 1.416514, val_loss: 2.215168 ,Tr_acc: 0.919101 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 390:Tr_loss: 1.414380, val_loss: 2.215016 ,Tr_acc: 0.919476 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 391:Tr_loss: 1.412253, val_loss: 2.216000 ,Tr_acc: 0.919850 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 392:Tr_loss: 1.410130, val_loss: 2.215819 ,Tr_acc: 0.920225 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 393:Tr_loss: 1.408014, val_loss: 2.216826 ,Tr_acc: 0.920974 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 394:Tr_loss: 1.405904, val_loss: 2.216640 ,Tr_acc: 0.921723 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 395:Tr_loss: 1.403801, val_loss: 2.217721 ,Tr_acc: 0.922472 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 396:Tr_loss: 1.401704, val_loss: 2.217456 ,Tr_acc: 0.923221 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 397:Tr_loss: 1.399612, val_loss: 2.218566 ,Tr_acc: 0.923970 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 398:Tr_loss: 1.397528, val_loss: 2.218305 ,Tr_acc: 0.924719 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 399:Tr_loss: 1.395449, val_loss: 2.219496 ,Tr_acc: 0.925843 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 400:Tr_loss: 1.393377, val_loss: 2.219183 ,Tr_acc: 0.926217 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 401:Tr_loss: 1.391312, val_loss: 2.220399 ,Tr_acc: 0.926592 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 402:Tr_loss: 1.389252, val_loss: 2.220077 ,Tr_acc: 0.926966 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 403:Tr_loss: 1.387198, val_loss: 2.221272 ,Tr_acc: 0.928464 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 404:Tr_loss: 1.385153, val_loss: 2.221014 ,Tr_acc: 0.928839 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 405:Tr_loss: 1.383113, val_loss: 2.222192 ,Tr_acc: 0.929213 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 406:Tr_loss: 1.381081, val_loss: 2.221940 ,Tr_acc: 0.930712 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 407:Tr_loss: 1.379055, val_loss: 2.223127 ,Tr_acc: 0.930337 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 408:Tr_loss: 1.377035, val_loss: 2.222872 ,Tr_acc: 0.933333 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 409:Tr_loss: 1.375023, val_loss: 2.224083 ,Tr_acc: 0.932584 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 410:Tr_loss: 1.373019, val_loss: 2.223812 ,Tr_acc: 0.935581 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 411:Tr_loss: 1.371022, val_loss: 2.225109 ,Tr_acc: 0.933708 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 412:Tr_loss: 1.369031, val_loss: 2.224723 ,Tr_acc: 0.937828 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 413:Tr_loss: 1.367044, val_loss: 2.226122 ,Tr_acc: 0.936704 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 414:Tr_loss: 1.365067, val_loss: 2.225671 ,Tr_acc: 0.938202 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 415:Tr_loss: 1.363093, val_loss: 2.227119 ,Tr_acc: 0.937079 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 416:Tr_loss: 1.361127, val_loss: 2.226674 ,Tr_acc: 0.938951 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 417:Tr_loss: 1.359165, val_loss: 2.228116 ,Tr_acc: 0.938951 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 418:Tr_loss: 1.357212, val_loss: 2.227688 ,Tr_acc: 0.940449 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 419:Tr_loss: 1.355263, val_loss: 2.229110 ,Tr_acc: 0.940075 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 420:Tr_loss: 1.353322, val_loss: 2.228737 ,Tr_acc: 0.941199 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 421:Tr_loss: 1.351390, val_loss: 2.230147 ,Tr_acc: 0.940824 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 422:Tr_loss: 1.349467, val_loss: 2.229758 ,Tr_acc: 0.942322 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 423:Tr_loss: 1.347548, val_loss: 2.231192 ,Tr_acc: 0.941199 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 424:Tr_loss: 1.345638, val_loss: 2.230795 ,Tr_acc: 0.943071 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 425:Tr_loss: 1.343734, val_loss: 2.232288 ,Tr_acc: 0.942322 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 426:Tr_loss: 1.341837, val_loss: 2.231806 ,Tr_acc: 0.944195 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 427:Tr_loss: 1.339947, val_loss: 2.233410 ,Tr_acc: 0.944569 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 428:Tr_loss: 1.338062, val_loss: 2.232905 ,Tr_acc: 0.945318 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 429:Tr_loss: 1.336187, val_loss: 2.234538 ,Tr_acc: 0.945693 , val_acc: 0.502000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 430:Tr_loss: 1.334319, val_loss: 2.233928 ,Tr_acc: 0.947191 , val_acc: 0.504000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 431:Tr_loss: 1.332458, val_loss: 2.235681 ,Tr_acc: 0.945693 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 432:Tr_loss: 1.330602, val_loss: 2.235015 ,Tr_acc: 0.947566 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 433:Tr_loss: 1.328756, val_loss: 2.236821 ,Tr_acc: 0.947940 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 434:Tr_loss: 1.326914, val_loss: 2.236108 ,Tr_acc: 0.948689 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 435:Tr_loss: 1.325078, val_loss: 2.237923 ,Tr_acc: 0.949064 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 436:Tr_loss: 1.323250, val_loss: 2.237276 ,Tr_acc: 0.950187 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 437:Tr_loss: 1.321431, val_loss: 2.239068 ,Tr_acc: 0.949438 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 438:Tr_loss: 1.319620, val_loss: 2.238379 ,Tr_acc: 0.950936 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 439:Tr_loss: 1.317816, val_loss: 2.240254 ,Tr_acc: 0.951685 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 440:Tr_loss: 1.316020, val_loss: 2.239533 ,Tr_acc: 0.953184 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 441:Tr_loss: 1.314232, val_loss: 2.241450 ,Tr_acc: 0.952809 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 442:Tr_loss: 1.312451, val_loss: 2.240629 ,Tr_acc: 0.953933 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 443:Tr_loss: 1.310676, val_loss: 2.242652 ,Tr_acc: 0.954682 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 444:Tr_loss: 1.308909, val_loss: 2.241801 ,Tr_acc: 0.956554 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 445:Tr_loss: 1.307150, val_loss: 2.243890 ,Tr_acc: 0.955431 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 446:Tr_loss: 1.305395, val_loss: 2.242952 ,Tr_acc: 0.957303 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 447:Tr_loss: 1.303647, val_loss: 2.245072 ,Tr_acc: 0.956554 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 448:Tr_loss: 1.301905, val_loss: 2.244154 ,Tr_acc: 0.958427 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 449:Tr_loss: 1.300174, val_loss: 2.246278 ,Tr_acc: 0.958052 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 450:Tr_loss: 1.298449, val_loss: 2.245321 ,Tr_acc: 0.959176 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 451:Tr_loss: 1.296733, val_loss: 2.247558 ,Tr_acc: 0.958801 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 452:Tr_loss: 1.295022, val_loss: 2.246527 ,Tr_acc: 0.960300 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 453:Tr_loss: 1.293319, val_loss: 2.248782 ,Tr_acc: 0.960300 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 454:Tr_loss: 1.291622, val_loss: 2.247768 ,Tr_acc: 0.962172 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 455:Tr_loss: 1.289934, val_loss: 2.250047 ,Tr_acc: 0.962547 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 456:Tr_loss: 1.288252, val_loss: 2.248982 ,Tr_acc: 0.962921 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 457:Tr_loss: 1.286582, val_loss: 2.251342 ,Tr_acc: 0.963296 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 458:Tr_loss: 1.284917, val_loss: 2.250217 ,Tr_acc: 0.963296 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 459:Tr_loss: 1.283257, val_loss: 2.252677 ,Tr_acc: 0.963670 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 460:Tr_loss: 1.281603, val_loss: 2.251447 ,Tr_acc: 0.964045 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 461:Tr_loss: 1.279958, val_loss: 2.253953 ,Tr_acc: 0.964419 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 462:Tr_loss: 1.278319, val_loss: 2.252748 ,Tr_acc: 0.964419 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 463:Tr_loss: 1.276692, val_loss: 2.255265 ,Tr_acc: 0.965169 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 464:Tr_loss: 1.275066, val_loss: 2.253985 ,Tr_acc: 0.965543 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 465:Tr_loss: 1.273449, val_loss: 2.256573 ,Tr_acc: 0.966667 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 466:Tr_loss: 1.271838, val_loss: 2.255290 ,Tr_acc: 0.966292 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 467:Tr_loss: 1.270234, val_loss: 2.257827 ,Tr_acc: 0.968165 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 468:Tr_loss: 1.268638, val_loss: 2.256621 ,Tr_acc: 0.969288 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 469:Tr_loss: 1.267054, val_loss: 2.259136 ,Tr_acc: 0.968914 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 470:Tr_loss: 1.265475, val_loss: 2.257888 ,Tr_acc: 0.969288 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 471:Tr_loss: 1.263902, val_loss: 2.260503 ,Tr_acc: 0.969288 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 472:Tr_loss: 1.262337, val_loss: 2.259163 ,Tr_acc: 0.970037 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 473:Tr_loss: 1.260783, val_loss: 2.261870 ,Tr_acc: 0.970037 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 474:Tr_loss: 1.259235, val_loss: 2.260445 ,Tr_acc: 0.970787 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 475:Tr_loss: 1.257696, val_loss: 2.263222 ,Tr_acc: 0.970037 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 476:Tr_loss: 1.256163, val_loss: 2.261734 ,Tr_acc: 0.971161 , val_acc: 0.500000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 477:Tr_loss: 1.254637, val_loss: 2.264655 ,Tr_acc: 0.971161 , val_acc: 0.498000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 478:Tr_loss: 1.253117, val_loss: 2.263009 ,Tr_acc: 0.971161 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 479:Tr_loss: 1.251604, val_loss: 2.266034 ,Tr_acc: 0.971536 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 480:Tr_loss: 1.250099, val_loss: 2.264316 ,Tr_acc: 0.972285 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 481:Tr_loss: 1.248599, val_loss: 2.267383 ,Tr_acc: 0.971536 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 482:Tr_loss: 1.247107, val_loss: 2.265645 ,Tr_acc: 0.972659 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 483:Tr_loss: 1.245620, val_loss: 2.268746 ,Tr_acc: 0.971910 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 484:Tr_loss: 1.244141, val_loss: 2.266990 ,Tr_acc: 0.973783 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 485:Tr_loss: 1.242666, val_loss: 2.270065 ,Tr_acc: 0.973408 , val_acc: 0.492000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 486:Tr_loss: 1.241201, val_loss: 2.268365 ,Tr_acc: 0.974532 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 487:Tr_loss: 1.239740, val_loss: 2.271434 ,Tr_acc: 0.973408 , val_acc: 0.492000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 488:Tr_loss: 1.238291, val_loss: 2.269707 ,Tr_acc: 0.975281 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 489:Tr_loss: 1.236846, val_loss: 2.272783 ,Tr_acc: 0.974532 , val_acc: 0.492000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 490:Tr_loss: 1.235416, val_loss: 2.271060 ,Tr_acc: 0.975655 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 491:Tr_loss: 1.233986, val_loss: 2.274192 ,Tr_acc: 0.974906 , val_acc: 0.492000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 492:Tr_loss: 1.232571, val_loss: 2.272411 ,Tr_acc: 0.976030 , val_acc: 0.496000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 493:Tr_loss: 1.231159, val_loss: 2.275660 ,Tr_acc: 0.975281 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 494:Tr_loss: 1.229754, val_loss: 2.273741 ,Tr_acc: 0.976404 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 495:Tr_loss: 1.228357, val_loss: 2.277088 ,Tr_acc: 0.976030 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 496:Tr_loss: 1.226965, val_loss: 2.275111 ,Tr_acc: 0.976779 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 497:Tr_loss: 1.225579, val_loss: 2.278514 ,Tr_acc: 0.976030 , val_acc: 0.492000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 498:Tr_loss: 1.224204, val_loss: 2.276471 ,Tr_acc: 0.977154 , val_acc: 0.494000 , lr: 0.050000 , Reg:0.001000\n",
      "Epoch 499:Tr_loss: 1.222831, val_loss: 2.279991 ,Tr_acc: 0.977154 , val_acc: 0.492000 , lr: 0.050000 , Reg:0.001000\n"
     ]
    }
   ],
   "source": [
    "class NN_FC ():\n",
    "    def __init__ (self,n_layers):\n",
    "        self.n_layers= n_layers\n",
    "        self.h = []\n",
    "        self.Best_W = {} \n",
    "        self.Best_b = {} \n",
    "        self.Best_val_acc = 0\n",
    "    \n",
    "    def define_layers (self,h): \n",
    "        if (len(h) != self.n_layers):\n",
    "            print('the length is not suitable')\n",
    "        else:\n",
    "            self.h = h\n",
    "    \n",
    "    def run(self,training_data = training_data , training_labels = training_labels ,\n",
    "                validation_data = validation_data , validation_labels = validation_labels,\n",
    "                D=3072 , K=5 , lr = 1e-0 , reg = 1e-3 , epochs = 100):\n",
    "        n_layers = self.n_layers\n",
    "        reg = reg\n",
    "        learning_rate = lr\n",
    "        h = self.h\n",
    "        weights = {}\n",
    "        bias = {}\n",
    "        \n",
    "        weights[0] = np.random.randn(D,h[0])/ np.sqrt(D)\n",
    "        bias[0]= np.zeros((1,h[0]))\n",
    "        weights[n_layers]= np.random.randn(h[n_layers-1], K)/ np.sqrt(h[n_layers-1])\n",
    "        bias[n_layers]= np.zeros((1,K))\n",
    "        \n",
    "\n",
    "        for i in range (1,n_layers):\n",
    "            weights[i] = np.random.randn(h[i-1],h[i])/ np.sqrt(h[i-1])\n",
    "            bias[i]= np.zeros((1,h[i]))\n",
    "    \n",
    "        X = training_data.astype('float64')\n",
    "        y = training_labels\n",
    "\n",
    "\n",
    "        X -= np.mean (X , axis = 0 )\n",
    "        X/= 255.0\n",
    "\n",
    "        validation_d = validation_data.astype('float64')\n",
    "        validation_d -= np.mean (validation_data , axis = 0 )\n",
    "\n",
    "        validation_d /= 255.0\n",
    "        num_examples = X.shape[0]\n",
    "        all_loss=[]\n",
    "        all_validation_loss = []\n",
    "        prev_val_acc = 0\n",
    "        hidden_layers= {}\n",
    "        hidden_layers[0] = np.maximum(0, np.dot(X, weights[0]) + bias[0])\n",
    "        validation_hidden_layers={}\n",
    "        for epoch in range(epochs): #10 epochs\n",
    "\n",
    "            for i in range (1 , n_layers):\n",
    "                tmp_h = np.maximum(0, np.dot(hidden_layers[i-1], weights[i]) + bias[i])\n",
    "                hidden_layers[i] =  tmp_h\n",
    "\n",
    "                \n",
    "            scores = np.dot(hidden_layers[n_layers-1], weights[n_layers]) + bias[n_layers]\n",
    "            training_predicted_class = np.argmax(scores, axis=1)\n",
    "            training_acc = np.mean(training_predicted_class == y)\n",
    "            \n",
    "            validation_hidden_layers[0]= np.maximum(0, np.dot(validation_d, weights[0]) + bias[0]) # note, ReLU activation\n",
    "            #evaluate validation accuracy\n",
    "            for i in range (1 , n_layers):\n",
    "                validation_hidden_layers[i]= np.maximum(0, np.dot(validation_hidden_layers[i-1], weights[i]) + bias[i])\n",
    "            \n",
    "           \n",
    "            validation_scores = np.dot(validation_hidden_layers[n_layers-1], weights[n_layers]) + bias[n_layers]\n",
    "\n",
    "            validation_predicted_class = np.argmax(validation_scores, axis=1)\n",
    "            validation_acc = np.mean(validation_predicted_class == validation_labels)\n",
    "\n",
    "            \n",
    "            scores -= np.max(scores) \n",
    "            exp_scores = np.exp(scores)\n",
    "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) \n",
    "\n",
    "           \n",
    "            correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "            data_loss = np.sum(correct_logprobs)/num_examples\n",
    "            reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2) + 0.5*reg*np.sum(W3*W3) \n",
    "            loss = data_loss + reg_loss\n",
    "            all_loss.append(loss)\n",
    "\n",
    "            # compute validation scores \n",
    "            validation_scores -= np.max(validation_scores) # to avoid numerical blowup\n",
    "            exp_validation_scores = np.exp(validation_scores)\n",
    "\n",
    "            validation_probs = exp_validation_scores / np.sum(exp_validation_scores, axis=1, keepdims=True) \n",
    "            validation_num_examples = validation_data.shape[0]\n",
    "            validation_correct_logprobs = -np.log(validation_probs[range(validation_num_examples),validation_labels])\n",
    "            \n",
    "            validation_data_loss = np.sum(validation_correct_logprobs)/validation_num_examples\n",
    "            validation_loss = validation_data_loss + reg_loss\n",
    "            all_validation_loss.append(validation_loss)\n",
    "\n",
    "            \n",
    "            print (\"Epoch %d: Tr_loss: %f, val_loss: %f ,Tr_acc: %f ,val_acc: %f ,lr: %f ,Reg:%f\" % (epoch, loss , validation_loss , training_acc , validation_acc , learning_rate , reg ))\n",
    "\n",
    "            dscores = probs\n",
    "            dscores[range(num_examples),y] -= 1\n",
    "            dscores /= num_examples\n",
    "            \n",
    "            dweights = {}\n",
    "            dbias = {}\n",
    "            dhidden = {}\n",
    "\n",
    "            dweights[n_layers] = (np.dot (hidden_layers[n_layers-1].T , dscores))\n",
    "            dbias[n_layers] = (np.sum(dscores, axis=0, keepdims=True))\n",
    "            dhidden[n_layers-1] =(np.dot(dscores , weights[n_layers].T))\n",
    "            if (hidden_layer[n_layers-1].any() <= 0):\n",
    "                dhidden[n_layers-1] = 0\n",
    "            for i in reversed(range (1,n_layers)):\n",
    "                dweights[i] =np.dot (hidden_layers[i-1].T , dhidden[i]) \n",
    "                dbias[i]=np.sum(dhidden[i], axis=0, keepdims=True)\n",
    "                dhidden[i-1] = np.dot(dhidden[i], weights[i].T)\n",
    "                if(hidden_layers[i-1].any()<=0):\n",
    "                    dhidden[i-1]=0\n",
    "            \n",
    "            dweights[0]= np.dot(X.T , dhidden[0])\n",
    "            dbias[0]=np.sum(dhidden[0] , axis = 0 , keepdims=True)\n",
    "   \n",
    "            for i in range ( 0 , n_layers+1):\n",
    "            \n",
    "                dweights[i] += reg * weights[i]\n",
    "\n",
    "            if (validation_acc > prev_val_acc):\n",
    "                prev_val_acc = validation_acc\n",
    "                Best_W = copy.deepcopy(weights)\n",
    "                Best_b = copy.deepcopy(bias)\n",
    "                Best_val_acc = copy.deepcopy(validation_acc)\n",
    "\n",
    "            for  i in range (0 , n_layers +1 ):\n",
    "                weights[i] += -learning_rate * dweights[i]\n",
    "                bias[i] += -learning_rate * dbias[i]\n",
    "                \n",
    "\n",
    "NN = NN_FC (2)\n",
    "NN.define_layers ([1000,1000])\n",
    "NN.run(lr= 5e-03 , epochs = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca1c3eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_loss ,label = 'training Loss')\n",
    "plt.plot (all_validation_loss , label='validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0463969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy is : 54.40 %\n"
     ]
    }
   ],
   "source": [
    "#running test set and calculating the ACCR\n",
    "testing_d = testing_data.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('testing accuracy is : %.2f ' % (np.mean(predicted_class == testing_labels)*100) +'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89ab7f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daisy accuracy: 36.00 %\n",
      "dandelion accuracy: 73.00 %\n",
      "roses accuracy: 42.00 %\n",
      "sunflowers accuracy: 68.00 %\n",
      "tulips accuracy: 53.00 %\n"
     ]
    }
   ],
   "source": [
    "#calculating individual classes and calculating the CCRNA\n",
    "#daisy\n",
    "testing_d = daisy.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('Daisy accuracy: %.2f ' % (np.mean(predicted_class == daisy_lbls)*100) +'%')\n",
    "testing_d = dandelion.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('dandelion accuracy: %.2f ' % (np.mean(predicted_class == dandelion_lbls)*100) +'%')\n",
    "testing_d = roses.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('roses accuracy: %.2f ' % (np.mean(predicted_class == roses_lbls)*100) +'%')\n",
    "testing_d = sunflowers.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('sunflowers accuracy: %.2f ' % (np.mean(predicted_class == sunflowers_lbls)*100) +'%')\n",
    "testing_d = tulips.astype('float64')\n",
    "testing_d -= np.mean (testing_data , axis = 0)\n",
    "testing_d /= 255.0\n",
    "testing_hidden_layer = np.maximum(0, np.dot(testing_d, Best_W) + Best_b) # note, ReLU activation\n",
    "testing_hidden_layer2= np.maximum(0, np.dot(testing_hidden_layer, Best_W2) + Best_b2) # note, ReLU activation\n",
    "testing_scores = np.dot(testing_hidden_layer2, Best_W3) + Best_b3\n",
    "\n",
    "predicted_class = np.argmax(testing_scores, axis=1)\n",
    "\n",
    "print ('tulips accuracy: %.2f ' % (np.mean(predicted_class == tulips_lbls)*100) +'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
